{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1068190",
   "metadata": {},
   "source": [
    "# Ejercicio 9\n",
    "\n",
    "Utilizando los ejemplos del archivo AUTOS.csv genere un modelo utilizando un multiperceptrón para predecir el precio del auto (atributo price) y la cantidad de millas por galón en ruta (MPG-highway) en función del resto de los atributos.  \n",
    "\n",
    "Recuerde completar los valores faltantes, utilizar normalización y dividir el dataset en entrenamiento y validación (80/20).  \n",
    "\n",
    "Realice 20 ejecuciones independientes de cada configuración seleccionada calculando las épocas promedio y el error cuadrático medio (ECM).  \n",
    "\n",
    "Analice los resultados y respalde las afirmaciones referidas a los resultados obtenidos.  \n",
    "Utilice un máximo de 1000 épocas con lotes de 50 e implemente una parada temprana con paciencia de 15.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f49fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATA_PATH = \"./Data/\"\n",
    "\n",
    "# ...existing code...\n",
    "# leer marcando '?' como NA\n",
    "data = pd.read_csv(DATA_PATH + 'autos.csv', na_values=['?'])\n",
    "\n",
    "data = data.replace('?', np.nan)\n",
    "# print(data.isnull().sum())\n",
    "cols = [\"normalized-losses\", \"bore\", \"stroke\", \"horsepower\", \"peak-rpm\", \"price\"]\n",
    "\n",
    "for col in cols:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "    data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "data = data.select_dtypes(include = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"])\n",
    "\n",
    "target_columns = [12, 14]  # mpg-highway y price\n",
    "Y_raw = data.iloc[:, target_columns].values\n",
    "X_raw = data.drop(data.columns[target_columns], axis=1).values\n",
    "\n",
    "data_scaler, target_scaler = StandardScaler(), StandardScaler()\n",
    "\n",
    "X_raw = data_scaler.fit_transform(X_raw)\n",
    "Y_raw = target_scaler.fit_transform(Y_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "815be315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow está configurado para usar CUDA (GPU).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifica si hay GPUs disponibles y configura para usar CUDA si es posible\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Configura TensorFlow para que use solo la memoria necesaria en la GPU\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"TensorFlow está configurado para usar CUDA (GPU).\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "else:\n",
    "  print(\"No se detectaron GPUs. TensorFlow usará la CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd6ea1",
   "metadata": {},
   "source": [
    "# Inciso A\n",
    "\n",
    "Complete la siguiente tabla y realice un análisis de los valores obtenidos:\n",
    "\n",
    "**Tabla 1 — Épocas promedio**\n",
    "\n",
    "| Optimizador | tanh | sigmoid | ReLU | LeakyReLU |\n",
    "| ----------- | ---- | ------- | ---- | --------- |\n",
    "| SGD         |      |         |      |           |\n",
    "| RMSProp     |      |         |      |           |\n",
    "| Adam        |      |         |      |           |\n",
    "\n",
    "**Tabla 2 — ECM promedio**\n",
    "\n",
    "| Optimizador | tanh | sigmoid | ReLU | LeakyReLU |\n",
    "| ----------- | ---- | ------- | ---- | --------- |\n",
    "| SGD         |      |         |      |           |\n",
    "| RMSProp     |      |         |      |           |\n",
    "| Adam        |      |         |      |           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd44be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f55946339c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f55946339c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Config -> Optimizador: sgd, Activación: tanh\n",
      " - Promedio de épocas: 553.00\n",
      " - Promedio de MSE (validación): 0.34327\n",
      " - Promedio R2 score: 0.60711\n",
      "\n",
      "Config -> Optimizador: sgd, Activación: sigmoid\n",
      " - Promedio de épocas: 946.20\n",
      " - Promedio de MSE (validación): 0.69019\n",
      " - Promedio R2 score: 0.26313\n",
      "\n",
      "Config -> Optimizador: sgd, Activación: relu\n",
      " - Promedio de épocas: 513.95\n",
      " - Promedio de MSE (validación): 0.47382\n",
      " - Promedio R2 score: 0.48242\n",
      "\n",
      "Config -> Optimizador: rmsprop, Activación: tanh\n",
      " - Promedio de épocas: 348.20\n",
      " - Promedio de MSE (validación): 0.33540\n",
      " - Promedio R2 score: 0.60670\n",
      "\n",
      "Config -> Optimizador: rmsprop, Activación: sigmoid\n",
      " - Promedio de épocas: 819.70\n",
      " - Promedio de MSE (validación): 0.34679\n",
      " - Promedio R2 score: 0.58862\n",
      "\n",
      "Config -> Optimizador: rmsprop, Activación: relu\n",
      " - Promedio de épocas: 374.10\n",
      " - Promedio de MSE (validación): 0.43187\n",
      " - Promedio R2 score: 0.52173\n",
      "\n",
      "Config -> Optimizador: adam, Activación: tanh\n",
      " - Promedio de épocas: 374.60\n",
      " - Promedio de MSE (validación): 0.30471\n",
      " - Promedio R2 score: 0.64796\n",
      "\n",
      "Config -> Optimizador: adam, Activación: sigmoid\n",
      " - Promedio de épocas: 777.30\n",
      " - Promedio de MSE (validación): 0.34202\n",
      " - Promedio R2 score: 0.59165\n",
      "\n",
      "Config -> Optimizador: adam, Activación: relu\n",
      " - Promedio de épocas: 316.55\n",
      " - Promedio de MSE (validación): 0.34140\n",
      " - Promedio R2 score: 0.57338\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "RUNS_PER_CONFIG = 20\n",
    "MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 50\n",
    "TEST_SIZE = 0.2\n",
    "PATIENCE = 15\n",
    "\n",
    "ENTRADAS = X_raw.shape[1]\n",
    "SALIDAS = Y_raw.shape[1]\n",
    "\n",
    "# Separar datos\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_raw, Y_raw, test_size=TEST_SIZE, random_state=42\n",
    ")\n",
    "\n",
    "optimizers = ['sgd', 'rmsprop', 'adam']\n",
    "activations = ['tanh', 'sigmoid', 'relu']\n",
    "\n",
    "def get_optimizer(name):\n",
    "    if name == 'sgd':\n",
    "        return SGD()\n",
    "    elif name == 'rmsprop':\n",
    "        return RMSprop()\n",
    "    elif name == 'adam':\n",
    "        return Adam()\n",
    "\n",
    "for opt in optimizers:\n",
    "    for act in activations:\n",
    "        epochs_list = []\n",
    "        mse_list = []\n",
    "        r2_list = []\n",
    "        f1_list = []\n",
    "\n",
    "        for run in range(RUNS_PER_CONFIG):\n",
    "            # Definir modelo\n",
    "            model = Sequential()\n",
    "            model.add(Input(shape=(ENTRADAS,)))\n",
    "            model.add(Dense(6, activation=act))\n",
    "            model.add(Dense(3, activation=act))\n",
    "            model.add(Dense(SALIDAS))\n",
    "\n",
    "            model.compile(optimizer=get_optimizer(opt),\n",
    "                          loss='mae', metrics=['mae', 'mse'])\n",
    "\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=PATIENCE, restore_best_weights=True\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                x=X_train, y=Y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=MAX_EPOCHS,\n",
    "                validation_data=(X_test, Y_test),\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            epochs_list.append(len(history.epoch))\n",
    "\n",
    "            # Evaluar métricas de Keras\n",
    "            loss, mae_val, mse_val = model.evaluate(X_test, Y_test, verbose=0)\n",
    "            mse_list.append(mse_val)\n",
    "\n",
    "            # Predicciones para métricas externas\n",
    "            Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "            # R2 score\n",
    "            r2_list.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "            # F1 score (si es clasificación binaria)\n",
    "            # Y_test_bin = np.round(Y_test)\n",
    "            # Y_pred_bin = np.round(Y_pred)\n",
    "            # f1_list.append(f1_score(Y_test_bin, Y_pred_bin))\n",
    "\n",
    "        mean_epochs = np.mean(epochs_list)\n",
    "        mean_mse = np.mean(mse_list)\n",
    "        mean_r2 = np.mean(r2_list)\n",
    "        # mean_f1 = np.mean(f1_list)\n",
    "\n",
    "        print(f\"\\nConfig -> Optimizador: {opt}, Activación: {act}\")\n",
    "        print(f\" - Promedio de épocas: {mean_epochs:.2f}\")\n",
    "        print(f\" - Promedio de MSE (validación): {mean_mse:.5f}\")\n",
    "        print(f\" - Promedio R2 score: {mean_r2:.5f}\")\n",
    "        # print(f\" - Promedio F1 score: {mean_f1:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc32411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config -> Optimizador: sgd, Activación: LeakyReLU\n",
      " - Promedio de épocas: 350.30\n",
      " - Promedio de MSE (validación): 0.30844\n",
      " - Promedio R2 score: 0.60200\n",
      "\n",
      "Config -> Optimizador: rmsprop, Activación: LeakyReLU\n",
      " - Promedio de épocas: 298.85\n",
      " - Promedio de MSE (validación): 0.26678\n",
      " - Promedio R2 score: 0.63860\n",
      "\n",
      "Config -> Optimizador: adam, Activación: LeakyReLU\n",
      " - Promedio de épocas: 287.40\n",
      " - Promedio de MSE (validación): 0.23358\n",
      " - Promedio R2 score: 0.68720\n"
     ]
    }
   ],
   "source": [
    "from tabnanny import verbose\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "RUNS_PER_CONFIG = 20\n",
    "MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 50\n",
    "TEST_SIZE = 0.2\n",
    "PATIENCE = 15\n",
    "\n",
    "ENTRADAS = X_raw.shape[1]\n",
    "SALIDAS = Y_raw.shape[1]\n",
    "\n",
    "# Separar datos\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_raw, Y_raw, test_size=TEST_SIZE, random_state=42\n",
    ")\n",
    "\n",
    "optimizers = ['sgd', 'rmsprop', 'adam']\n",
    "activation = 'leaky_relu'\n",
    "\n",
    "def get_optimizer(name):\n",
    "    if name == 'sgd':\n",
    "        return SGD()\n",
    "    elif name == 'rmsprop':\n",
    "        return RMSprop()\n",
    "    elif name == 'adam':\n",
    "        return Adam()\n",
    "\n",
    "for opt in optimizers:\n",
    "    epochs_list = []\n",
    "    mse_list = []\n",
    "    r2_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for run in range(RUNS_PER_CONFIG):\n",
    "        # Definir modelo con LeakyReLU\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(ENTRADAS,)))\n",
    "        model.add(Dense(6))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dense(3))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dense(SALIDAS))\n",
    "\n",
    "        model.compile(optimizer=get_optimizer(opt),\n",
    "                      loss='mae', metrics=['mae', 'mse'])\n",
    "\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=PATIENCE, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            x=X_train, y=Y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=MAX_EPOCHS,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Guardar número de épocas y MSE validación\n",
    "        epochs_list.append(len(history.epoch))\n",
    "        loss, mae_val, mse_val = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        mse_list.append(mse_val)\n",
    "\n",
    "        # Predicciones\n",
    "        Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "        # R2 score (regresión)\n",
    "        r2_list.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "        # F1 score (si es clasificación binaria, convertir a 0/1)\n",
    "        # Y_test_bin = np.round(Y_test)\n",
    "        # Y_pred_bin = np.round(Y_pred)\n",
    "        # f1_list.append(f1_score(Y_test_bin, Y_pred_bin))\n",
    "\n",
    "    # Promedios\n",
    "    mean_epochs = np.mean(epochs_list)\n",
    "    mean_mse = np.mean(mse_list)\n",
    "    mean_r2 = np.mean(r2_list)\n",
    "    # mean_f1 = np.mean(f1_list)\n",
    "\n",
    "    print(f\"\\nConfig -> Optimizador: {opt}, Activación: LeakyReLU\")\n",
    "    print(f\" - Promedio de épocas: {mean_epochs:.2f}\")\n",
    "    print(f\" - Promedio de MSE (validación): {mean_mse:.5f}\")\n",
    "    print(f\" - Promedio R2 score: {mean_r2:.5f}\")\n",
    "    # print(f\" - Promedio F1 score: {mean_f1:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
